{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "from spacy.symbols import nsubj, nsubjpass, det, dobj, pobj, prep, root, neg, agent, cc, conj, acl, xcomp, punct, VERB\n",
    "import io\n",
    "import datetime\n",
    "import json\n",
    "import codecs\n",
    "import csv\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "from ipywidgets import IntProgress\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "#------------------------------------- Load data ---------------------------------------#\n",
    "\n",
    "def load_csv_to_dict(csvfile, language, language_position):\n",
    "    \"\"\"\n",
    "     Creates a dictionary with word tokens as keys, and their 'features' as values.\n",
    "    \"\"\"\n",
    "    dictionary = dict()\n",
    "    data = [row for row in csv.reader(open('%s' % csvfile, encoding='utf-8-sig'), delimiter=\";\")]\n",
    "    header = data.pop(0)       # first row of csv = header.\n",
    "    header.pop(0)       # remove first element of header.\n",
    "    for row in data:\n",
    "        if (language in row[language_position].lower()) or (language == \"all\"):\n",
    "            key = row.pop(0)    # first element in row = key\n",
    "            for index, element in enumerate(row):\n",
    "                dictionary.setdefault(key.lower(), {})[header[index].lower()] = element.lower()\n",
    "    return (dictionary)\n",
    "\n",
    "\n",
    "def setup_spacy(user_text, nlp):\n",
    "    \"\"\"\n",
    "     Lowers the user text and sets the spacy pipeline.\n",
    "    \"\"\"\n",
    "    user_text = user_text.lower()\n",
    "    return nlp(user_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------- Matcher Algorithm ---------------------------------------#\n",
    "\n",
    "def find_match_dict(user_text, dicts, user_highlights, reinforced_highlights, nlp):\n",
    "    \"\"\"\n",
    "     This function loops over all word tokens within the user text,\n",
    "     to find a match with a key within a set of dictionaries.\n",
    "     It returns a dictionary with the matches string possition and attributes.\n",
    "    \"\"\"\n",
    "    doc = setup_spacy(user_text, nlp)\n",
    "    results = dict()\n",
    "    for key in dicts.keys():    # iterates over the three datatypes.\n",
    "        part_dict = dict()\n",
    "        sent_counter = 0\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:       # iterates over all spacy tokens in the text.\n",
    "                if token.text in dicts[key] or token.lemma_ in dicts[key]:\n",
    "                    id = uuid.uuid4()\n",
    "                    if check_user_highlight(user_highlights, \n",
    "                                            key, \n",
    "                                            target = {\"text\": token.text, \n",
    "                                                      \"index\" : (token.idx, \n",
    "                                                                 token.idx+len(token.text))}) == True:\n",
    "                        reinforced_highlights[str(id)] = {\"text\": token.text, \n",
    "                                                          \"index\" : (token.idx, token.idx+len(token.text))}\n",
    "                    else:\n",
    "                        noun_update = False\n",
    "                        for chunk in doc.noun_chunks:\n",
    "                            if (((chunk.root.idx == token.idx) and (chunk.text != token.text)) and (key == \"roles\")):\n",
    "                                noun_update = True\n",
    "                                part_dict[str(id)] = {\"text\": chunk.text,\n",
    "                                                      \"index\" : (chunk.start_char, chunk.end_char), \n",
    "                                                      \"sent\": sent_counter}\n",
    "                        \n",
    "                        if noun_update == False:\n",
    "                            part_dict[str(id)] = {\"text\": token.text,\n",
    "                                                  \"index\" : (token.idx, token.idx+len(token.text)), \n",
    "                                                  \"sent\": sent_counter}\n",
    "            sent_counter = sent_counter+1\n",
    "        results[key] = part_dict\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlight Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------- Intersection Checker ---------------------------------------#\n",
    "\n",
    "def check_user_highlight(user_highlights, datatype, target):\n",
    "    \"\"\"\n",
    "     This function checks if a target annotation already exists in the user_highlights.\n",
    "     Best result is exact matches. Otherwise it checks for match in index intervals and texts.\n",
    "    \"\"\"\n",
    "    match = False\n",
    "    interval_match = False\n",
    "    text_match = False\n",
    "    if datatype in user_highlights.keys():  # Check if the datatype exists.\n",
    "        for key, value in user_highlights[datatype].items():\n",
    "            if datatype == 'activities':  # Activities are nested in the dict.\n",
    "                val_start, val_end = int(value['label']['index'][0]), int(value['label']['index'][1])  # Set index boundaries.\n",
    "                tar_start, tar_end = int(target['verb']['index'][0]), int(target['verb']['index'][1]) \n",
    "                \n",
    "                if 'text' in target['verb'].keys():  # Check if it contains text.\n",
    "                    target_text = target['verb']['text']\n",
    "                    value_text = value['label']['text']\n",
    "            else:  # All other datatypes:\n",
    "                val_start, val_end = int(value['index'][0]), int(value['index'][1])  # Set index boundaries.\n",
    "                tar_start, tar_end = int(target['index'][0]), int(target['index'][1]) \n",
    "                \n",
    "                if 'text' in target.keys():  # Check if it contains text.\n",
    "                    target_text = target['text']\n",
    "                    value_text = value['text']\n",
    "\n",
    "            if (val_start, val_end) == (tar_start, tar_end):  # Checking if exact matches.\n",
    "                match = True\n",
    "\n",
    "                if target_text is not None:  # Failsafe: Checking if the strings are identical.\n",
    "                    error = 'Match Error: Index match, but strings are different: {}'.format((target_text, value_text))\n",
    "                    assert (target_text == value_text), error\n",
    "                  \n",
    "            else:   # Checks if the two intervals overlap.\n",
    "                t_ran = range(tar_start, tar_end+1)\n",
    "                v_ran = range(val_start, val_end+1)\n",
    "\n",
    "                if len(list(set(t_ran) & set(v_ran))) >= 1:   \n",
    "                    match = True\n",
    "\n",
    "                    if target_text is not None:  # Failsafe: Checking string intervals.\n",
    "                        error = 'Match Error: Index match, but strings are different: {}'.format((target_text, value_text))\n",
    "                        assert (target_text in value_text or value_text in target_text), error\n",
    "\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------- Rule-based Algorithm ---------------------------------------#\n",
    "\n",
    "\n",
    "def voice_detector(token):\n",
    "    \"\"\"\n",
    "    Function for identifying Voice type\n",
    "    Identifying passive or active voice in a given sentence.\n",
    "    \"\"\"\n",
    "    doc = token.doc\n",
    "    idx = token.i\n",
    "\n",
    "    # passive voice:\n",
    "    if token.tag_ == 'VBN' and (doc[idx-1].lemma_ == 'be' or\n",
    "                                doc[idx-2].lemma_ == 'be' or\n",
    "                                doc[idx-1].lemma_ == 'have'):\n",
    "        voice = 'passive'\n",
    "        return voice\n",
    "\n",
    "    # active voice:\n",
    "    elif token.dep_ == \"ROOT\" or (token.tag_ == 'VBG' and (token.dep != prep and \n",
    "         token.dep != acl)) or token.tag_ == 'VBD' or (token.tag_ == 'VBZ' \n",
    "         and token.dep_ != 'aux'):\n",
    "        \n",
    "        voice = 'active'\n",
    "        return voice\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def instance_check(idx):\n",
    "    \"\"\"\n",
    "    Check whether index is of datatype int. \n",
    "    \"\"\"\n",
    "    if isinstance(idx, int):\n",
    "        return idx\n",
    "    else:\n",
    "        return int(idx)\n",
    "    \n",
    "def dependencies_check(children_deps):\n",
    "    \"\"\"\n",
    "    Check dependencies of a token's children.\n",
    "    \"\"\"\n",
    "    deps = [nsubj, nsubjpass, dobj]\n",
    "    count = 0\n",
    "    for child_dep in children_deps:\n",
    "        if child_dep in deps:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def extract_indexes(token, voice):\n",
    "    \"\"\"\n",
    "    Exctract index span for activity text.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    children = [child for child in token.children]\n",
    "    children_deps = [child.dep for child in token.children]\n",
    "    length = len(children)\n",
    "    \n",
    "    for idx, child in enumerate(children):\n",
    "        \n",
    "        if token.dep != conj:\n",
    "            count = dependencies_check(children_deps)\n",
    "            if count == 0: # if there are no children (dependency tags), break the loop\n",
    "                break\n",
    "        \n",
    "        if voice == 'active':\n",
    "            if child.dep == nsubj:\n",
    "                start_idx = instance_check(child.i + 1) # don't include subject in span\n",
    "                indexes.append(start_idx)\n",
    "            elif child.dep == dobj:\n",
    "                end_idx = instance_check(child.i + 1)\n",
    "                indexes.append(end_idx)\n",
    "                break\n",
    "        elif voice == 'passive':\n",
    "            if child.dep == nsubj:\n",
    "                start_idx = instance_check(child.i + 1)\n",
    "                indexes.append(start_idx) # don't include subject in span\n",
    "            elif child.dep == nsubjpass:\n",
    "                start_idx = child.i # include object in span (nsubjpass in passive voice is an object and not a subject)\n",
    "                indexes.append(start_idx)\n",
    "            elif child.dep == agent: # corresponding pobj is the actual subject in a passive voice sentence\n",
    "                end_idx = child.i\n",
    "                indexes.append(end_idx)\n",
    "                break\n",
    "            elif child.dep == dobj:\n",
    "                end_idx = instance_check(child.i + 1)\n",
    "                indexes.append(end_idx)\n",
    "                break\n",
    "        \n",
    "        if idx == length - 1 and (child.dep == conj and children[idx-1].dep != cc):\n",
    "            # handle conjuncts separately (as their own activities), \n",
    "            # if the previous token is not a conjunction (e.g., or, and)\n",
    "            pass\n",
    "        elif idx == length - 1 and (child.dep == dobj or (child.dep == conj and children[idx-1].dep == cc) or \n",
    "            child.dep == xcomp):\n",
    "            end_idx = instance_check(child.i + 1)\n",
    "            indexes.append(end_idx)\n",
    "        elif idx == length - 1 and child.dep != dobj:\n",
    "            end_idx = child.i\n",
    "            indexes.append(end_idx)\n",
    "    \n",
    "    return indexes \n",
    "\n",
    "\n",
    "def create_act_desc(token, voice):\n",
    "    \"\"\"\n",
    "    Takes a token and a voice type (passive or active) and returns a dictionary consisting of an activity text \n",
    "    and its corresponding indexes.\n",
    "    \"\"\"\n",
    "    act_text = None\n",
    "    doc = token.doc\n",
    "    idx = token.i\n",
    "    indexes = extract_indexes(token, voice)\n",
    "\n",
    "    if indexes != list():\n",
    "        # create activity description\n",
    "        if len(indexes) == 1:\n",
    "            if indexes[0] < idx:\n",
    "                start_idx = indexes[0]\n",
    "                end_idx = idx+1\n",
    "                act_text = doc[start_idx:end_idx]\n",
    "            elif indexes[0] > idx:\n",
    "                start_idx = idx\n",
    "                end_idx = indexes[0]\n",
    "                act_text = doc[start_idx:end_idx]\n",
    "\n",
    "        elif len(indexes) >= 2: \n",
    "            indexes = sorted(indexes, reverse = False) # sort in ascending order \n",
    "            start_idx = indexes[0]\n",
    "            end_idx = indexes[1]\n",
    "            if end_idx <= idx:\n",
    "                act_text = doc[start_idx:idx+1]\n",
    "            else:\n",
    "                act_text = doc[start_idx:end_idx]\n",
    "        \n",
    "    return {\"text\": str(act_text), \"index\": (act_text[0].idx, act_text[0].idx+len(str(act_text)))}\n",
    "    \n",
    "    \n",
    "def process_token(token):\n",
    "    \"\"\"\n",
    "    Given a token, its important attributes are returned.\n",
    "    Returnes a set-dictionary.\n",
    "    \"\"\"\n",
    "    prep = False\n",
    "    neg = False\n",
    "    children = [child for child in token.children]\n",
    "    for child in children:\n",
    "\n",
    "        if child.dep_ == \"prep\" or child.dep_ == \"advcl\":\n",
    "            prep = True\n",
    "        if child.dep_ == \"neg\":\n",
    "            neg = True\n",
    "\n",
    "    conjuncts = token.conjuncts\n",
    "    \n",
    "    if neg == True:\n",
    "        return {\"text\": token.text,\n",
    "            \"negated\": neg,\n",
    "            \"index\": (token.idx, token.idx+len(token.text))}\n",
    "    else:\n",
    "        return {\"text\": token.text,\n",
    "                \"index\": (token.idx, token.idx+len(token.text))}\n",
    "\n",
    "\n",
    "def identify_act(token, sent):\n",
    "    \"\"\"\n",
    "    Find object and subject for activity.\n",
    "    Return token values, activity voice and activity description.\n",
    "    \"\"\"\n",
    "    act_stat = False # used to check if there has been found an activity.\n",
    "    act = {}\n",
    "    # Find if activty is passive or active voice:\n",
    "    act[\"voice\"] = voice_detector(token)\n",
    "    # Get token values for activity word:\n",
    "    act[\"verb\"] = process_token(token)\n",
    "    act[\"sent\"] = sent\n",
    "    # passive voice handling:\n",
    "    if act[\"voice\"] == 'passive':\n",
    "        for child in token.children:\n",
    "            if child.dep == nsubj:\n",
    "                act[\"subject\"] = process_token(child)\n",
    "                act_stat = True\n",
    "\n",
    "            elif child.dep == nsubjpass or child.dep == dobj:\n",
    "                act[\"object\"] = process_token(child)\n",
    "                act_stat = True\n",
    "\n",
    "    # active Voice handling:\n",
    "    elif act[\"voice\"] == 'active':\n",
    "        for child in token.children:\n",
    "            if child.dep == nsubj:\n",
    "                act[\"subject\"] = process_token(child)\n",
    "                act_stat = True\n",
    "\n",
    "            elif child.dep == dobj:\n",
    "                act[\"object\"] = process_token(child)\n",
    "                act_stat = True\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if act_stat == True:\n",
    "        act[\"activity_label\"] = create_act_desc(token, act[\"voice\"])\n",
    "        return act\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_sent(sent, idx, user_highlights, reinforced_highlights):\n",
    "    \"\"\"\n",
    "    Checks a sentence, if it meets the criteria for having one or more activities.\n",
    "    Takes a sent, and returns a dictionary of activities.\n",
    "    \"\"\"\n",
    "    acts_stat = False\n",
    "    acts = {}\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'VERB': \n",
    "            # If a verb is found, try and construct activity\n",
    "            activity = identify_act(token, idx)\n",
    "            id = uuid.uuid4()\n",
    "            if identify_act(token, sent) != None:\n",
    "                if check_user_highlight(user_highlights, 'activities', activity) == True:\n",
    "                    reinforced_highlights[str(id)] = {\"text\": token.text, \"index\" : (token.idx, token.idx+len(token.text))}\n",
    "                else:\n",
    "                    acts[str(id)] = activity\n",
    "                    acts_stat = True\n",
    "    if acts_stat == True:\n",
    "        return acts\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def activity_recognition(desc, user_highlights, reinforced_highlights, nlp):\n",
    "    \"\"\"\n",
    "    Top iteration, taking care of the document is split into sentences.\n",
    "    Takes a text, and returns all identified activities not already highlighted by the user.\n",
    "    Output is structured in 3 layers: 1. Sentences ( 2. Activities ( 3. Tokens)\n",
    "    \"\"\"\n",
    "    doc = setup_spacy(desc, nlp)\n",
    "    sent_acts = {}\n",
    "\n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        analysis = process_sent(sent, i, user_highlights, reinforced_highlights)\n",
    "        if analysis != None:\n",
    "            sent_acts = merge_two_dicts(sent_acts, analysis)\n",
    "\n",
    "    return sent_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------- API & Data Input ---------------------------------------#\n",
    "\n",
    "# load data from csv into dictionaries:\n",
    "languages = [\"all\", \"en\", \"da\", \"pt\"] # all = all languages\n",
    "datatypes = [[\"roles\", 3], [\"relations\", 3], [\"alias\", 1]]  # type, language_position\n",
    "datasets = dict()\n",
    "for language in languages:\n",
    "    temp = dict()\n",
    "    for datatype in datatypes:\n",
    "        temp[datatype[0]] = load_csv_to_dict(\"%s.csv\" % datatype[0], \n",
    "                                             language, \n",
    "                                             datatype[1])\n",
    "    datasets[language] = temp\n",
    "    \n",
    "    \n",
    "def postJsonHandler(event, context):\n",
    "    \"\"\"\n",
    "    Handles the API call created by AWS API Gateway.\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    plain_text = event[\"plain_text\"] # Handles the input request, by breaking it into parts. \n",
    "    user_language = event[\"language\"]\n",
    "    user_highlights = event[\"highlights\"]\n",
    "\n",
    "\n",
    "    reg = re.findall(r\"^\\w+\",user_language) # language handling:\n",
    "    if reg and reg[0] in languages:\n",
    "        reg_language = reg[0]\n",
    "\n",
    "    dict_results = {}   # structures results in a combined dictionary:\n",
    "    reinforced_highlights = dict()\n",
    "    dict_results[\"reinforced_highlights\"] = dict()\n",
    "    dict_results[\"entity_recognition\"] = find_match_dict(plain_text, \n",
    "                                                         datasets[reg_language], \n",
    "                                                         user_highlights, \n",
    "                                                         reinforced_highlights, \n",
    "                                                         nlp)\n",
    "    if reg_language == \"en\" or user_language == \"all\":\n",
    "        activities = activity_recognition(plain_text, user_highlights, reinforced_highlights, nlp)\n",
    "        dict_results[\"activity_recognition\"] = activities\n",
    "        #dict_results[\"reinforced_highlights\"] = activities[1]\n",
    "\n",
    "    return (dict_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "test = re.split(r'(?<=\\.) ', text)\n",
    "\n",
    "df = pd.DataFrame(test, columns=['Text'])\n",
    "sentences = df.Text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Generate cluster, start index and end index\n",
    "for cluster in doc._.coref_clusters:\n",
    "    for reference in cluster:\n",
    "        #print(reference, (reference.start, reference.end))\n",
    "        references.append(reference)\n",
    "        clusters_idx_start.append(reference.start)\n",
    "        clusters_idx_end.append(reference.end)\n",
    "        \n",
    "references  = [i.text for i in references]\n",
    "df = pd.DataFrame(references, columns=['Mentions'])\n",
    "df_1 = pd.DataFrame(clusters_idx_start, columns=[\"Start_Idx\"])\n",
    "df_2 = pd.DataFrame(clusters_idx_end, columns=[\"End_Idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Start_Idx\"] = df_1[\"Start_Idx\"]\n",
    "df[\"End_Idx\"] = df_2[\"End_Idx\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_sent = tokenizer.encode(sent) # Add '[CLS]' and '[SEP]'\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=30, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "input_ids = torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = BertConfig.from_pretrained('Models/Relations/config.json')\n",
    "model_1 = BertForSequenceClassification.from_pretrained('Models/Relations/pytorch_model.bin', config=config_1)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "model_1.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_1(input_ids)\n",
    "    #print(output)\n",
    "\n",
    "logits = output[0]\n",
    "logits = logits.detach().cpu().numpy()\n",
    "\n",
    "# Store predictions and true labels\n",
    "predictions.append(logits)\n",
    "\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "sent_pred = zip(sentences, predictions)\n",
    "df_pred = pd.DataFrame(sent_pred, columns=[\"Sentence\", \"Tag\"]) \n",
    "df_pred['Tag'].replace(0, 'Non-Relation',inplace=True)\n",
    "df_pred['Tag'].replace(1, 'Relation',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_pred[(df_pred.Tag == \"Relation\")]\n",
    "sentences_2 = df_2.Sentence.values\n",
    "\n",
    "input_ids_2 = []\n",
    "\n",
    "for sent in sentences_2:\n",
    "    encoded_sent_2 = tokenizer.encode(sent) # Add '[CLS]' and '[SEP]'\n",
    "    input_ids_2.append(encoded_sent_2)\n",
    "\n",
    "input_ids_2 = pad_sequences(input_ids_2, maxlen=30, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "input_ids_2 = torch.tensor(input_ids_2)\n",
    "\n",
    "%%capture\n",
    "config_2 = BertConfig.from_pretrained('Models/Relations_Type/config.json')\n",
    "model_2 = BertForSequenceClassification.from_pretrained('Models/Relations_Type/pytorch_model.bin', config=config_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2 = []\n",
    "\n",
    "model_2.eval()\n",
    "with torch.no_grad():\n",
    "    output_2 = model_2(input_ids_2)\n",
    "\n",
    "logits_2 = output_2[0]\n",
    "logits_2 = logits_2.detach().cpu().numpy()\n",
    "\n",
    "# Store predictions and true labels\n",
    "predictions_2.append(logits_2)\n",
    "print('Predictions are DONE.')\n",
    "\n",
    "predictions_2 = np.argmax(logits_2, axis=1)\n",
    "\n",
    "sent_pred_2 = zip(sentences_2, predictions_2)\n",
    "df_pred_2 = pd.DataFrame(sent_pred_2, columns=[\"Sentence\", \"Tag\"]) \n",
    "df_pred_2['Tag'].replace(0, 'Conditions',inplace=True)\n",
    "df_pred_2['Tag'].replace(1, 'Non-Relation',inplace=True)\n",
    "df_pred_2['Tag'].replace(2, 'Response',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_enrichment(text, rel_pred, type_pred, model, method=\"intersection\"):\n",
    "    \"\"\"\n",
    "    Takes relations from BERT Model, and process them into sentences. \n",
    "    And outputs the comparison with RB.\n",
    "    \"\"\"\n",
    "    doc = setup_spacy(text, model)\n",
    "    results = []\n",
    "    for idx, sentence in enumerate(doc.sents):\n",
    "        hl_txt = rel_pred.values[idx][0]\n",
    "        hl_type = type_pred.values[idx][1]\n",
    "        hl_sent = setup_spacy(hl_txt, spacy.load('en_core_web_sm'))\n",
    "        hl_input = {'text': hl_sent.text, 'index': (0, hl_sent[len(hl_sent)-1].idx)}\n",
    "        input = {  \n",
    "            \"graphid\": \"0\",\n",
    "            \"userid\": \"0\",\n",
    "            \"organizationid\": \"0\",\n",
    "            \"language\": \"en-US\",\n",
    "            \"plain_text\": sentence.text,\n",
    "            \"highlights\": {'roles': {}, 'relations': {0: hl_input}, \"activities\":{}}}\n",
    "        output = postJsonHandler(input,())\n",
    "        results.append([True, hl_type, output['reinforced_highlights']])\n",
    "        print(output['reinforced_highlights'])\n",
    "    return results\n",
    "\n",
    "stats = (relation_enrichment(text, df_pred, df_pred_2, spacy.load('en_core_web_sm')))\n",
    "#for el in stats:\n",
    "#    print(\"Pred True for sent: \" + str(el[0]), \"Type: \" + el[1], \"Reinforced: \", el[2]['reinforced_highlights'])\n",
    "    \n",
    "stats_df = pd.DataFrame((stats), columns =['Prediction', 'Type', 'Reinforced'])\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roles & Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "# join bpe split tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        new_labels.append(tag_values[label_idx])\n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pred = zip(new_tokens, new_labels)\n",
    "df_pred = pd.DataFrame(sent_pred, columns=[\"Sentence\", \"Tag\"]) \n",
    "\n",
    "# Get indexes \n",
    "test_sentence = \"HI, this is is a test\"\n",
    "test_sentence_res = nlp(test_sentence)\n",
    "doc = test_sentence_res\n",
    "sent_index = [(token.idx, token.idx + len(token.text)) for token in doc]\n",
    "df_sent = pd.DataFrame(sent_index, columns=[\"Start_Idx\", \"End_Idx\"]) \n",
    "\n",
    "# Combine dataframes \n",
    "df_pred[\"Start_Idx\"] = df_sent[\"Start_Idx\"]\n",
    "df_pred[\"End_Idx\"] = df_sent[\"End_Idx\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "nlp1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
