{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.symbols import nsubj, nsubjpass, det, dobj, pobj, prep, root, neg, agent, cc, conj, acl, xcomp, punct, VERB\n",
    "import io\n",
    "import datetime\n",
    "import json\n",
    "import codecs\n",
    "import csv\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#------------------------------------- Load data ---------------------------------------#\n",
    "\n",
    "def load_csv_to_dict(csvfile, language, language_position):\n",
    "    \"\"\"\n",
    "     Creates a dictionary with word tokens as keys, and their 'features' as values.\n",
    "    \"\"\"\n",
    "    dictionary = dict()\n",
    "    data = [row for row in csv.reader(open('%s' % csvfile, encoding='utf-8-sig'), delimiter=\";\")]\n",
    "    header = data.pop(0)       # first row of csv = header.\n",
    "    header.pop(0)       # remove first element of header.\n",
    "    for row in data:\n",
    "        if (language in row[language_position].lower()) or (language == \"all\"):\n",
    "            key = row.pop(0)    # first element in row = key\n",
    "            for index, element in enumerate(row):\n",
    "                dictionary.setdefault(key.lower(), {})[header[index].lower()] = element.lower()\n",
    "    return (dictionary)\n",
    "\n",
    "\n",
    "def setup_spacy(user_text, nlp):\n",
    "    \"\"\"\n",
    "     Lowers the user text and sets the spacy pipeline.\n",
    "    \"\"\"\n",
    "    user_text = user_text.lower()\n",
    "    return nlp(user_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------- Filter input data ---------------------------------------#\n",
    "\n",
    "def check_user_highlight(user_highlights, datatype, target):\n",
    "    \"\"\"\n",
    "     This function checks if a target annotation already exists in the user_highlights.\n",
    "     Best result is exact matches. Otherwise it checks for match in index intervals and texts.\n",
    "    \"\"\"\n",
    "    match = False\n",
    "    interval_match = False\n",
    "    text_match = False\n",
    "    if datatype in user_highlights.keys():  # Check if the datatype exists.\n",
    "        for key, value in user_highlights[datatype].items():\n",
    "            if datatype == 'activities':  # Activities are nested in the dict.\n",
    "                val_start, val_end = int(value['label']['index'][0]), int(value['label']['index'][1])  # Set index boundaries.\n",
    "                tar_start, tar_end = int(target['verb']['index'][0]), int(target['verb']['index'][1]) \n",
    "                \n",
    "                if 'text' in target['verb'].keys():  # Check if it contains text.\n",
    "                    target_text = target['verb']['text']\n",
    "                    value_text = value['label']['text']\n",
    "            else:  # All other datatypes:\n",
    "                val_start, val_end = int(value['index'][0]), int(value['index'][1])  # Set index boundaries.\n",
    "                tar_start, tar_end = int(target['index'][0]), int(target['index'][1]) \n",
    "                \n",
    "                if 'text' in target.keys():  # Check if it contains text.\n",
    "                    target_text = target['text']\n",
    "                    value_text = value['text']\n",
    "\n",
    "            if (val_start, val_end) == (tar_start, tar_end):  # Checking if exact matches.\n",
    "                match = True\n",
    "\n",
    "                if target_text is not None:  # Failsafe: Checking if the strings are identical.\n",
    "                    error = 'Match Error: Index match, but strings are different: {}'.format((target_text, value_text))\n",
    "                    assert (target_text == value_text), error\n",
    "                  \n",
    "            else:   # Checks if the two intervals overlap.\n",
    "                t_ran = range(tar_start, tar_end+1)\n",
    "                v_ran = range(val_start, val_end+1)\n",
    "\n",
    "                if len(list(set(t_ran) & set(v_ran))) >= 1:   \n",
    "                    match = True\n",
    "\n",
    "                    if target_text is not None:  # Failsafe: Checking string intervals.\n",
    "                        error = 'Match Error: Index match, but strings are different: {}'.format((target_text, value_text))\n",
    "                        assert (target_text in value_text or value_text in target_text), error\n",
    "\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------- Entity Recognition ---------------------------------------#\n",
    "\n",
    "def find_match_dict(user_text, dicts, user_highlights, reinforced_highlights, nlp):\n",
    "    \"\"\"\n",
    "     This function loops over all word tokens within the user text,\n",
    "     to find a match with a key within a set of dictionaries.\n",
    "     It returns a dictionary with the matches string possition and attributes.\n",
    "    \"\"\"\n",
    "    doc = setup_spacy(user_text, nlp)\n",
    "    results = dict()\n",
    "    for key in dicts.keys():    # iterates over the three datatypes.\n",
    "        part_dict = dict()\n",
    "        sent_counter = 0\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:       # iterates over all spacy tokens in the text.\n",
    "                if token.text in dicts[key] or token.lemma_ in dicts[key]:\n",
    "                    id = uuid.uuid4()\n",
    "                    if check_user_highlight(user_highlights, \n",
    "                                            key, \n",
    "                                            target = {\"text\": token.text, \n",
    "                                                      \"index\" : (token.idx, \n",
    "                                                                 token.idx+len(token.text))}) == True:\n",
    "                        reinforced_highlights[str(id)] = {\"text\": token.text, \n",
    "                                                          \"index\" : (token.idx, token.idx+len(token.text))}\n",
    "                    else:\n",
    "                        noun_update = False\n",
    "                        for chunk in doc.noun_chunks:\n",
    "                            if (((chunk.root.idx == token.idx) and (chunk.text != token.text)) and (key == \"roles\")):\n",
    "                                noun_update = True\n",
    "                                part_dict[str(id)] = {\"text\": chunk.text,\n",
    "                                                      \"index\" : (chunk.start_char, chunk.end_char), \n",
    "                                                      \"sent\": sent_counter}\n",
    "                        \n",
    "                        if noun_update == False:\n",
    "                            part_dict[str(id)] = {\"text\": token.text,\n",
    "                                                  \"index\" : (token.idx, token.idx+len(token.text)), \n",
    "                                                  \"sent\": sent_counter}\n",
    "            sent_counter = sent_counter+1\n",
    "        results[key] = part_dict\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------- Activity Recognition ---------------------------------------#\n",
    "\n",
    "\n",
    "def voice_detector(token):\n",
    "    \"\"\"\n",
    "    Function for identifying Voice type\n",
    "    Identifying passive or active voice in a given sentence.\n",
    "    \"\"\"\n",
    "    doc = token.doc\n",
    "    idx = token.i\n",
    "\n",
    "    # passive voice:\n",
    "    if token.tag_ == 'VBN' and (doc[idx-1].lemma_ == 'be' or\n",
    "                                doc[idx-2].lemma_ == 'be' or\n",
    "                                doc[idx-1].lemma_ == 'have'):\n",
    "        voice = 'passive'\n",
    "        return voice\n",
    "\n",
    "    # active voice:\n",
    "    elif token.dep_ == \"ROOT\" or (token.tag_ == 'VBG' and (token.dep != prep and \n",
    "         token.dep != acl)) or token.tag_ == 'VBD' or (token.tag_ == 'VBZ' \n",
    "         and token.dep_ != 'aux'):\n",
    "        \n",
    "        voice = 'active'\n",
    "        return voice\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def instance_check(idx):\n",
    "    \"\"\"\n",
    "    Check whether index is of datatype int. \n",
    "    \"\"\"\n",
    "    if isinstance(idx, int):\n",
    "        return idx\n",
    "    else:\n",
    "        return int(idx)\n",
    "    \n",
    "def dependencies_check(children_deps):\n",
    "    \"\"\"\n",
    "    Check dependencies of a token's children.\n",
    "    \"\"\"\n",
    "    deps = [nsubj, nsubjpass, dobj]\n",
    "    count = 0\n",
    "    for child_dep in children_deps:\n",
    "        if child_dep in deps:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def extract_indexes(token, voice):\n",
    "    \"\"\"\n",
    "    Exctract index span for activity text.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    children = [child for child in token.children]\n",
    "    children_deps = [child.dep for child in token.children]\n",
    "    length = len(children)\n",
    "    \n",
    "    for idx, child in enumerate(children):\n",
    "        \n",
    "        if token.dep != conj:\n",
    "            count = dependencies_check(children_deps)\n",
    "            if count == 0: # if there are no children (dependency tags), break the loop\n",
    "                break\n",
    "        \n",
    "        if voice == 'active':\n",
    "            if child.dep == nsubj:\n",
    "                start_idx = instance_check(child.i + 1) # don't include subject in span\n",
    "                indexes.append(start_idx)\n",
    "            elif child.dep == dobj:\n",
    "                end_idx = instance_check(child.i + 1)\n",
    "                indexes.append(end_idx)\n",
    "                break\n",
    "        elif voice == 'passive':\n",
    "            if child.dep == nsubj:\n",
    "                start_idx = instance_check(child.i + 1)\n",
    "                indexes.append(start_idx) # don't include subject in span\n",
    "            elif child.dep == nsubjpass:\n",
    "                start_idx = child.i # include object in span (nsubjpass in passive voice is an object and not a subject)\n",
    "                indexes.append(start_idx)\n",
    "            elif child.dep == agent: # corresponding pobj is the actual subject in a passive voice sentence\n",
    "                end_idx = child.i\n",
    "                indexes.append(end_idx)\n",
    "                break\n",
    "            elif child.dep == dobj:\n",
    "                end_idx = instance_check(child.i + 1)\n",
    "                indexes.append(end_idx)\n",
    "                break\n",
    "        \n",
    "        if idx == length - 1 and (child.dep == conj and children[idx-1].dep != cc):\n",
    "            # handle conjuncts separately (as their own activities), \n",
    "            # if the previous token is not a conjunction (e.g., or, and)\n",
    "            pass\n",
    "        elif idx == length - 1 and (child.dep == dobj or (child.dep == conj and children[idx-1].dep == cc) or \n",
    "            child.dep == xcomp):\n",
    "            end_idx = instance_check(child.i + 1)\n",
    "            indexes.append(end_idx)\n",
    "        elif idx == length - 1 and child.dep != dobj:\n",
    "            end_idx = child.i\n",
    "            indexes.append(end_idx)\n",
    "    \n",
    "    try:\n",
    "        assert len(indexes) <= 2 # indexes list shall not contain more than two integers\n",
    "    except:\n",
    "        print(\"Problem occurred for {}: Indexes list contains too many elements (> 2)\".format(token))\n",
    "        \n",
    "    return indexes \n",
    "\n",
    "\n",
    "def create_act_desc(token, voice):\n",
    "    \"\"\"\n",
    "    Takes a token and a voice type (passive or active) and returns a dictionary consisting of an activity text \n",
    "    and its corresponding indexes.\n",
    "    \"\"\"\n",
    "    act_text = None\n",
    "    doc = token.doc\n",
    "    idx = token.i\n",
    "    indexes = extract_indexes(token, voice)\n",
    "\n",
    "    if indexes != list():\n",
    "        # create activity description\n",
    "        if len(indexes) == 1:\n",
    "            if indexes[0] < idx:\n",
    "                start_idx = indexes[0]\n",
    "                end_idx = idx+1\n",
    "                act_text = doc[start_idx:end_idx]\n",
    "            elif indexes[0] > idx:\n",
    "                start_idx = idx\n",
    "                end_idx = indexes[0]\n",
    "                act_text = doc[start_idx:end_idx]\n",
    "\n",
    "        elif len(indexes) >= 2: \n",
    "            indexes = sorted(indexes, reverse = False) # sort in ascending order \n",
    "            start_idx = indexes[0]\n",
    "            end_idx = indexes[1]\n",
    "            if end_idx <= idx:\n",
    "                act_text = doc[start_idx:idx+1]\n",
    "            else:\n",
    "                act_text = doc[start_idx:end_idx]\n",
    "        \n",
    "    return {\"text\": str(act_text), \"index\": (act_text[0].idx, act_text[0].idx+len(str(act_text)))}\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "def process_token(token):\n",
    "    \"\"\"\n",
    "    Given a token, its important attributes are returned.\n",
    "    Returnes a set-dictionary.\n",
    "    \"\"\"\n",
    "    prep = False\n",
    "    neg = False\n",
    "    children = [child for child in token.children]\n",
    "    for child in children:\n",
    "\n",
    "        if child.dep_ == \"prep\" or child.dep_ == \"advcl\":\n",
    "            prep = True\n",
    "        if child.dep_ == \"neg\":\n",
    "            neg = True\n",
    "\n",
    "    conjuncts = token.conjuncts\n",
    "    \n",
    "    if neg == True:\n",
    "        return {\"text\": token.text,\n",
    "            \"negated\": neg,\n",
    "            \"index\": (token.idx, token.idx+len(token.text))}\n",
    "    else:\n",
    "        return {\"text\": token.text,\n",
    "                \"index\": (token.idx, token.idx+len(token.text))}\n",
    "\n",
    "\n",
    "def identify_act(token, sent):\n",
    "    \"\"\"\n",
    "    Find object and subject for activity.\n",
    "    Return token values, activity voice and activity description.\n",
    "    \"\"\"\n",
    "    act_stat = False # used to check if there has been found an activity.\n",
    "    act = {}\n",
    "    # Find if activty is passive or active voice:\n",
    "    act[\"voice\"] = voice_detector(token)\n",
    "    # Get token values for activity word:\n",
    "    act[\"verb\"] = process_token(token)\n",
    "    act[\"sent\"] = sent\n",
    "    # passive voice handling:\n",
    "    if act[\"voice\"] == 'passive':\n",
    "        for child in token.children:\n",
    "            if child.dep == nsubj:\n",
    "                act[\"subject\"] = process_token(child)\n",
    "                act_stat = True\n",
    "\n",
    "            elif child.dep == nsubjpass or child.dep == dobj:\n",
    "                act[\"object\"] = process_token(child)\n",
    "                act_stat = True\n",
    "\n",
    "    # active Voice handling:\n",
    "    elif act[\"voice\"] == 'active':\n",
    "        for child in token.children:\n",
    "            if child.dep == nsubj:\n",
    "                act[\"subject\"] = process_token(child)\n",
    "                act_stat = True\n",
    "\n",
    "            elif child.dep == dobj:\n",
    "                act[\"object\"] = process_token(child)\n",
    "                act_stat = True\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if act_stat == True:\n",
    "        act[\"activity_label\"] = create_act_desc(token, act[\"voice\"])\n",
    "        return act\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_sent(sent, idx, user_highlights, reinforced_highlights):\n",
    "    \"\"\"\n",
    "    Checks a sentence, if it meets the criteria for having one or more activities.\n",
    "    Takes a sent, and returns a dictionary of activities.\n",
    "    \"\"\"\n",
    "    acts_stat = False\n",
    "    acts = {}\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'VERB': \n",
    "            # If a verb is found, try and construct activity\n",
    "            activity = identify_act(token, idx)\n",
    "            id = uuid.uuid4()\n",
    "            if identify_act(token, sent) != None:\n",
    "                if check_user_highlight(user_highlights, 'activities', activity) == True:\n",
    "                    reinforced_highlights[str(id)] = {\"text\": token.text, \"index\" : (token.idx, token.idx+len(token.text))}\n",
    "                else:\n",
    "                    acts[str(id)] = activity\n",
    "                    acts_stat = True\n",
    "    if acts_stat == True:\n",
    "        return acts\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def activity_recognition(desc, user_highlights, reinforced_highlights, nlp):\n",
    "    \"\"\"\n",
    "    Top iteration, taking care of the document is split into sentences.\n",
    "    Takes a text, and returns all identified activities not already highlighted by the user.\n",
    "    Output is structured in 3 layers: 1. Sentences ( 2. Activities ( 3. Tokens)\n",
    "    \"\"\"\n",
    "    doc = setup_spacy(desc, nlp)\n",
    "    sent_acts = {}\n",
    "\n",
    "    #sents = list(doc.sents)\n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        analysis = process_sent(sent, i, user_highlights, reinforced_highlights)\n",
    "        if analysis != None:\n",
    "            sent_acts = merge_two_dicts(sent_acts, analysis)\n",
    "\n",
    "    return sent_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------- API & Data Input ---------------------------------------#\n",
    "\n",
    "# load data from csv into dictionaries:\n",
    "languages = [\"all\", \"en\", \"da\", \"pt\"] # all = all languages\n",
    "datatypes = [[\"roles\", 3], [\"relations\", 3], [\"alias\", 1]]  # type, language_position\n",
    "datasets = dict()\n",
    "for language in languages:\n",
    "    temp = dict()\n",
    "    for datatype in datatypes:\n",
    "        temp[datatype[0]] = load_csv_to_dict(\"%s.csv\" % datatype[0], \n",
    "                                             language, \n",
    "                                             datatype[1])\n",
    "    datasets[language] = temp\n",
    "    \n",
    "    \n",
    "def postJsonHandler(event, context):\n",
    "    \"\"\"\n",
    "    Handles the API call created by AWS API Gateway.\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    plain_text = event[\"plain_text\"] # Handles the input request, by breaking it into parts. \n",
    "    user_language = event[\"language\"]\n",
    "    user_highlights = event[\"highlights\"]\n",
    "\n",
    "\n",
    "    reg = re.findall(r\"^\\w+\",user_language) # language handling:\n",
    "    if reg and reg[0] in languages:\n",
    "        reg_language = reg[0]\n",
    "\n",
    "    dict_results = {}   # structures results in a combined dictionary:\n",
    "    reinforced_highlights = dict()\n",
    "    dict_results[\"reinforced_highlights\"] = dict()\n",
    "    dict_results[\"entity_recognition\"] = find_match_dict(plain_text, \n",
    "                                                         datasets[reg_language], \n",
    "                                                         user_highlights, \n",
    "                                                         reinforced_highlights, \n",
    "                                                         nlp)\n",
    "    if reg_language == \"en\" or user_language == \"all\":\n",
    "        activities = activity_recognition(plain_text, user_highlights, reinforced_highlights, nlp)\n",
    "        dict_results[\"activity_recognition\"] = activities\n",
    "        #dict_results[\"reinforced_highlights\"] = activities[1]\n",
    "\n",
    "    return (dict_results)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API calls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reinforced_highlights': {},\n",
       " 'entity_recognition': {'roles': {'4ded9f4a-2e2c-4204-acdb-16e2f4f0d740': {'text': 'another employee',\n",
       "    'index': (38, 54),\n",
       "    'sent': 0},\n",
       "   '7b56978a-d598-475a-b1f5-d600f29ecbf3': {'text': 'an office manager',\n",
       "    'index': (95, 112),\n",
       "    'sent': 1}},\n",
       "  'relations': {'faecf95b-c2cc-485d-9ab0-6414ba061c87': {'text': 'meanwhile',\n",
       "    'index': (85, 94),\n",
       "    'sent': 1}},\n",
       "  'alias': {}},\n",
       " 'activity_recognition': {'f108b920-0de7-44ba-b18a-b4853ba3f1f6': {'voice': 'active',\n",
       "   'verb': {'text': 'signs', 'index': (16, 21)},\n",
       "   'sent': 0,\n",
       "   'subject': {'text': 'employee', 'index': (7, 15)},\n",
       "   'object': {'text': 'work', 'index': (32, 36)},\n",
       "   'activity_label': {'text': 'signs the paper work', 'index': (16, 36)}},\n",
       "  '0f994251-00bb-423f-b1f6-2ff7eaf1498d': {'voice': 'active',\n",
       "   'verb': {'text': 'reads', 'index': (113, 118)},\n",
       "   'sent': 1,\n",
       "   'subject': {'text': 'manager', 'index': (105, 112)},\n",
       "   'object': {'text': 'email', 'index': (123, 128)},\n",
       "   'activity_label': {'text': 'reads the email', 'index': (113, 128)}}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {  \n",
    "    \"graphid\": \"0\",\n",
    "    \"userid\": \"0\",\n",
    "    \"organizationid\": \"0\",\n",
    "    # graphid, userid and organizationid is not currently affecting the results.\n",
    "    \n",
    "    \"language\": \"en-US\",\n",
    "    # the input for language loads different models. Currently we support en-*, da-*, Pt-*, and have an all option for development.\n",
    "        \n",
    "    \"plain_text\": \"If the employee signs the paper work, another employee sends an email to the office. Meanwhile an Office Manager reads the email.\",\n",
    "    # A plain text in UTF-8. Make sure there are no strange characters and spelling mistakes.\n",
    "        \n",
    "    \"highlights\": {'roles': {0: {'text': 'employee', 'index': (7, 15)}}, \n",
    "                   'relations': {0: {'text': 'if', 'index': (0, 10)}}, \n",
    "                   'activities': {0: {'root': {'text': 'send', 'index': (50, 55)}, \n",
    "                                      'label': {'text': 'sends an email', 'index': (55, 69)}}}} \n",
    "    #Highlights are grouped in types, and each each has an id (dict key). The index is mandatory, and text is optional as a failsafe. \n",
    "    # For activities, the label is mandatory and the root is optional. However the root is most important, and therefore if the root\n",
    "    # is not given, the label is analyzed in order to find the root.\n",
    "    }\n",
    "\n",
    "postJsonHandler(input, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict()\n",
    "csvfile = \"ex_processes.csv\"\n",
    "data = [row for row in csv.reader(open('%s' % csvfile, encoding='utf-8-sig'), delimiter=\",\")]\n",
    "header = [\"title\", \"text\"]\n",
    "for num, row in enumerate(data):\n",
    "    key = num    # first element in row = key\n",
    "    for index, element in enumerate(row):\n",
    "        dictionary.setdefault(key, {})[header[index].lower()] = element.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem occurred for finished: Indexes list contains too many elements (> 2)\n",
      "Problem occurred for finished: Indexes list contains too many elements (> 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------------------- Generate results from dataset ---------------------------------------#\n",
    "\n",
    "for id in dictionary:\n",
    "    input = {\"graphid\": str(id),\n",
    "             \"userid\": \"0\",\n",
    "             \"organizationid\": \"0\",\n",
    "             \"language\": \"en-US\",    \n",
    "             \"plain_text\": dictionary[id]['text'],\n",
    "             \"highlights\": {}\n",
    "            }\n",
    "    \n",
    "    output = postJsonHandler(input, ())\n",
    "    \n",
    "    # Handling Output - Create 3 csv files.\n",
    "    types = [\"roles\", \"relations\", \"activities\"]\n",
    "    \n",
    "    for ty in types:\n",
    "        if ((ty == \"roles\") or (ty == \"relations\")):\n",
    "            data = output[\"entity_recognition\"][ty]\n",
    "            \n",
    "            with open('external-{}.csv'.format('{}-{}'.format(str(id), ty)), 'a') as csv_file:\n",
    "                fieldnames = ['text', 'start', 'end']\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                for key in data:\n",
    "                    writer.writerow({'text': data[key][\"text\"], \n",
    "                                     'start': data[key][\"index\"][0], \n",
    "                                     'end': data[key][\"index\"][1]})\n",
    "        else:\n",
    "            data = output[\"activity_recognition\"]\n",
    "            \n",
    "            with open('external-{}.csv'.format('{}-{}'.format(str(id), ty)), 'a') as csv_file:\n",
    "                fieldnames = ['text', 'start', 'end']\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                for key in data:\n",
    "                        writer.writerow({'text': data[key][\"activity_label\"][\"text\"], \n",
    "                                         'start': data[key][\"activity_label\"][\"index\"][0], \n",
    "                                         'end': data[key][\"activity_label\"][\"index\"][1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, delimiter=\",\"):\n",
    "    \"\"\"Given a filename, data is loaded and returned in a list.\"\"\"\n",
    "    try:\n",
    "        data = [row for row in csv.reader(open('%s' % filename, encoding='utf-8-sig'), delimiter=delimiter)]\n",
    "    except:\n",
    "        data = []\n",
    "    return data\n",
    "\n",
    "\n",
    "def overlap(start1, end1, start2, end2):\n",
    "    \"\"\"Check if the range (start1, end1) overlap with (start2, end2)\"\"\"\n",
    "    return (end1 >= start2) and (end2 >= start1)\n",
    "\n",
    "\n",
    "def compare_results(A, B):\n",
    "    \"\"\"Function that takes two lists with row(text, start index, end index) and compares indexes.\"\"\"\n",
    "    matches = []\n",
    "    A_leftovers = []\n",
    "\n",
    "    # Compare A to B:\n",
    "    for A_row in A:\n",
    "        if A_row[1] == '':\n",
    "            pass\n",
    "        else:\n",
    "            A_start = int(A_row[1])\n",
    "            A_end = int(A_row[2])\n",
    "            match = False\n",
    "\n",
    "            for B_row in B:\n",
    "                if match == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    if B_row[1] == '':\n",
    "                        pass\n",
    "                    else:\n",
    "                        B_start = int(B_row[1])\n",
    "                        B_end = int(B_row[2]) \n",
    "\n",
    "                        if overlap(A_start, A_end, B_start, B_end) == True:\n",
    "                            matches.append([(A_start, A_end), (B_start, B_end), A_row[0], B_row[0]])\n",
    "                            match = True\n",
    "\n",
    "        if match == False:\n",
    "            A_leftovers.append([A_row[0], A_start, A_end])\n",
    "    return matches\n",
    "\n",
    "\n",
    "def create_statistics(list):\n",
    "    \"\"\"Given a list with pairs, compare each pair and return the results in a list.\"\"\"\n",
    "    statistics = []\n",
    "    for pair in list:\n",
    "        A = load_data(pair[0])\n",
    "        B = load_data(pair[1])\n",
    "        matches = compare_results(A, B)\n",
    "        statistics.append([\"{}\".format(pair[0]), len(A), len(matches)])\n",
    "    return statistics\n",
    "\n",
    "\n",
    "def create_list_pairs(filetemps, n):\n",
    "    pairs = []\n",
    "    for idx in range(1,n+1):\n",
    "        pairs.append([filetemps[0].format(idx), filetemps[1].format(idx)])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "#------------------------------------- Compare two result files ---------------------------------------#\n",
    "\n",
    "\n",
    "templates = [\"{}-activities.csv\", \"JM-{}-activities-ML.csv\"]\n",
    "\n",
    "def template(ty):\n",
    "    RB = \"{}-\" + str(ty) + \".csv\"\n",
    "    ML = \"{}-\" + str(ty) + \".csv\"\n",
    "    return [RB, ML]\n",
    "\n",
    "#templates = [templates[1], templates[0]]\n",
    "stats = create_statistics(create_list_pairs(template(\"activities\"), 37))\n",
    "\n",
    "stats_df = pd.DataFrame((stats), columns =['Filename', 'Elements in file', 'Matches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = create_statistics(create_list_pairs(template(\"roles\"), 37))\n",
    "stats_df = pd.DataFrame((stats), columns =['Filename', 'Elements in file', 'Matches'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "\n",
    "def get_user_annos(filename, annotation_type):\n",
    "    with open(filename) as fd:\n",
    "        doc = xmltodict.parse(fd.read())\n",
    "    output_dict = json.loads(json.dumps(doc))\n",
    "    highlights = []\n",
    "    lista = []\n",
    "    user_annos = []\n",
    "    for highlight in output_dict[\"dcrgraph\"][\"specification\"][\"resources\"][\"custom\"][\"highlighterMarkup\"][\"highlights\"][\"highlight\"]:\n",
    "\n",
    "        annos = [] \n",
    "        if highlight[\"@type\"] == annotation_type:\n",
    "            annos = []\n",
    "\n",
    "            for key, value in highlight[\"layers\"][\"layer\"][\"ranges\"][\"range\"].items():\n",
    "                if key == \"@start\":\n",
    "                    start = value\n",
    "                elif key == \"@end\":\n",
    "                    end = value\n",
    "                elif key == \"#text\":\n",
    "                    text = value\n",
    "            anno = [text,start,end]\n",
    "            user_annos.append(anno)\n",
    "    return user_annos\n",
    "\n",
    "def get_graph_name(filename):\n",
    "    with open(filename) as fd:\n",
    "        doc = xmltodict.parse(fd.read())\n",
    "    output_dict = json.loads(json.dumps(doc))\n",
    "    return output_dict[\"dcrgraph\"][\"@title\"]\n",
    "\n",
    "#------------------------------------- Compare user annotations against file ---------------------------------------#\n",
    "\n",
    "def template(ty):\n",
    "    RB = \"{}-\" + str(ty) + \".csv\"\n",
    "    ML = \"JM-{}-\" + str(ty) + \"-ML.csv\"\n",
    "    return [RB, ML]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------- Compare RB + ML with dataset ---------------------------------------#\n",
    "\n",
    "def compare_results_to_graph(file, annotation_type, method=\"intersection\"):\n",
    "    \"\"\"Compare annotations for rb, ml, or the intersection between them and an annotated graph.\"\"\"\n",
    "    \n",
    "    statistics = []\n",
    "    file_name = get_graph_name(file[1])\n",
    "    user_annos = get_user_annos(file[1], annotation_type[0])\n",
    "    \n",
    "    \n",
    "    targets = [\"external-{}-\".format(file[0]-1) + str(annotation_type[1]) + \".csv\", \n",
    "               \"JM-{}-\".format(file[0]) + str(annotation_type[1]) + \".csv\"]\n",
    "    A = load_data(targets[0])\n",
    "    B = load_data(targets[1])\n",
    "    matches = compare_results(A, B)\n",
    "    reordered_matches = []\n",
    "    for match in matches:\n",
    "        text = match[2]\n",
    "        start = match[0][0]\n",
    "        end = match[0][1]\n",
    "        reorder = [text, start, end]\n",
    "        reordered_matches.append(reorder)\n",
    "        \n",
    "        \n",
    "    if method == \"intersection\":    \n",
    "        prediction = reordered_matches\n",
    "        hits = compare_results(user_annos, prediction)\n",
    "        \n",
    "    elif method == \"rb\":\n",
    "        prediction = A\n",
    "        hits = compare_results(user_annos, prediction)\n",
    "        \n",
    "    elif method == \"ml\":\n",
    "        prediction = B\n",
    "        hits = compare_results(user_annos, prediction)\n",
    "    \n",
    "    gold = user_annos\n",
    "    \n",
    "    if len(hits) != 0:    \n",
    "        \n",
    "        precision = len(hits)/len(prediction)\n",
    "        recall = len(hits)/len(gold)\n",
    "        f1 = (2*precision*recall)/(precision+recall)\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "    statistics = [\"{} {}: {}\".format(file[0], file_name, ty[1]), \n",
    "                  len(prediction), \n",
    "                  len(gold), \n",
    "                  len(hits),\n",
    "                  precision,\n",
    "                  recall,\n",
    "                  f1]\n",
    "    return statistics\n",
    "\n",
    "def Average(lst): \n",
    "    if sum(lst) != 0:\n",
    "        return sum(lst) / len(lst) \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def classify_sents(prediction, file):\n",
    "    file_name = get_graph_name(file[1])\n",
    "    gold = file[2]\n",
    "    hits = len(prediction) - abs(gold - len(prediction))\n",
    "    if len(prediction) != 0:    \n",
    "        precision = (hits)/len(prediction)\n",
    "        recall = (hits)/(gold)\n",
    "        f1 = (2*precision*recall)/(precision+recall)\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "    statistics = [\"{} {}: {}\".format(file[0], file_name, \"relations\"), \n",
    "                  len(prediction), \n",
    "                  (gold), \n",
    "                  (hits),\n",
    "                  precision,\n",
    "                  recall,\n",
    "                  f1]\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg prec Score: 0.7625258564964449\n",
      "Avg recall Score: 0.7843316405816407\n",
      "Avg F1 Score: 0.7196487900950376\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Graph</th>\n",
       "      <th>True Positives</th>\n",
       "      <th>Prec</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 DCR: activities</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 DCR: roles</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 DCR: relations</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 van der : activities</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 van der : roles</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2 van der : relations</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 vander aa 3: activities</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3 vander aa 3: roles</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3 vander aa 3: relations</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4 DCR: activities</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4 DCR: roles</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4 DCR: relations</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5 Conduct Directions Hearing - CAiSE2020: acti...</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5 Conduct Directions Hearing - CAiSE2020: roles</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5 Conduct Directions Hearing - CAiSE2020: rela...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6 Finalice Sct Warranty Posession: activities</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6 Finalice Sct Warranty Posession: roles</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6 Finalice Sct Warranty Posession: relations</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7 BPMAI: Example 1: activities</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7 BPMAI: Example 1: roles</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.424242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7 BPMAI: Example 1: relations</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8 BPMAI: Example 2: activities</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8 BPMAI: Example 2: roles</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8 BPMAI: Example 2: relations</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9 BPMAI Example 3: activities</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9 BPMAI Example 3: roles</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9 BPMAI Example 3: relations</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10 #3 Manually: Expense Report: activities</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10 #3 Manually: Expense Report: roles</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10 #3 Manually: Expense Report: relations</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Filename  Predictions  Graph  \\\n",
       "0                                   1 DCR: activities            7      9   \n",
       "1                                        1 DCR: roles            2      2   \n",
       "2                                    1 DCR: relations            3      2   \n",
       "3                              2 van der : activities            6     12   \n",
       "4                                   2 van der : roles            6      3   \n",
       "5                               2 van der : relations            4      4   \n",
       "6                           3 vander aa 3: activities            4      4   \n",
       "7                                3 vander aa 3: roles            3      2   \n",
       "8                            3 vander aa 3: relations            2      1   \n",
       "9                                   4 DCR: activities            6      7   \n",
       "10                                       4 DCR: roles            5      5   \n",
       "11                                   4 DCR: relations            3      3   \n",
       "12  5 Conduct Directions Hearing - CAiSE2020: acti...            7      9   \n",
       "13    5 Conduct Directions Hearing - CAiSE2020: roles            2      2   \n",
       "14  5 Conduct Directions Hearing - CAiSE2020: rela...            4      4   \n",
       "15      6 Finalice Sct Warranty Posession: activities            9     11   \n",
       "16           6 Finalice Sct Warranty Posession: roles           16      8   \n",
       "17       6 Finalice Sct Warranty Posession: relations            0      6   \n",
       "18                     7 BPMAI: Example 1: activities           15     13   \n",
       "19                          7 BPMAI: Example 1: roles           20     13   \n",
       "20                      7 BPMAI: Example 1: relations           12      4   \n",
       "21                     8 BPMAI: Example 2: activities           14     14   \n",
       "22                          8 BPMAI: Example 2: roles           17      9   \n",
       "23                      8 BPMAI: Example 2: relations            7      5   \n",
       "24                      9 BPMAI Example 3: activities           13     16   \n",
       "25                           9 BPMAI Example 3: roles            6      3   \n",
       "26                       9 BPMAI Example 3: relations            7      6   \n",
       "27         10 #3 Manually: Expense Report: activities            1     16   \n",
       "28              10 #3 Manually: Expense Report: roles            6      7   \n",
       "29          10 #3 Manually: Expense Report: relations            8      7   \n",
       "\n",
       "    True Positives      Prec    Recall        F1  \n",
       "0                6  0.857143  0.666667  0.750000  \n",
       "1                2  1.000000  1.000000  1.000000  \n",
       "2                2  0.666667  1.000000  0.800000  \n",
       "3                7  1.166667  0.583333  0.777778  \n",
       "4                2  0.333333  0.666667  0.444444  \n",
       "5                4  1.000000  1.000000  1.000000  \n",
       "6                3  0.750000  0.750000  0.750000  \n",
       "7                1  0.333333  0.500000  0.400000  \n",
       "8                1  0.500000  1.000000  0.666667  \n",
       "9                7  1.166667  1.000000  1.076923  \n",
       "10               5  1.000000  1.000000  1.000000  \n",
       "11               3  1.000000  1.000000  1.000000  \n",
       "12               8  1.142857  0.888889  1.000000  \n",
       "13               2  1.000000  1.000000  1.000000  \n",
       "14               4  1.000000  1.000000  1.000000  \n",
       "15               9  1.000000  0.818182  0.900000  \n",
       "16               8  0.500000  1.000000  0.666667  \n",
       "17              -6  0.000000  0.000000  0.000000  \n",
       "18               5  0.333333  0.384615  0.357143  \n",
       "19               7  0.350000  0.538462  0.424242  \n",
       "20               4  0.333333  1.000000  0.500000  \n",
       "21               8  0.571429  0.571429  0.571429  \n",
       "22               7  0.411765  0.777778  0.538462  \n",
       "23               5  0.714286  1.000000  0.833333  \n",
       "24              11  0.846154  0.687500  0.758621  \n",
       "25               3  0.500000  1.000000  0.666667  \n",
       "26               6  0.857143  1.000000  0.923077  \n",
       "27               2  2.000000  0.125000  0.235294  \n",
       "28               4  0.666667  0.571429  0.615385  \n",
       "29               7  0.875000  1.000000  0.933333  "
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [[1, \"DE1.xml\", 2], [2, \"DE2.xml\", 4], [3, \"DE3.xml\", 1], [4, \"DE4.xml\", 3], [5, \"ATDP1.xml\", 4], [6, \"ATDP2.xml\", 6], [7, \"1.xml\", 4], \n",
    "        [8, \"2.xml\", 5], [9, \"3.xml\", 6], [10, \"4.xml\", 7]]                                            \n",
    "\n",
    "#test = [[1, \"DE1.xml\", 3]]\n",
    "\n",
    "#user_files = [[1, \"1.xml\"], [2, \"2.xml\"], [3, \"3.xml\"], [4, \"4.xml\"], [7, \"7.xml\"], \n",
    "#             [13, \"13.xml\"], [21, \"21.xml\"]]     \n",
    "\n",
    "stats = []\n",
    "for file in files:\n",
    "    for ty in [[\"activity\", \"activities\"]]:\n",
    "        stats.append(compare_results_to_graph(file, ty, \"intersection\"))\n",
    "    for ty in [[\"role\", \"roles\"]]:\n",
    "        stats.append(compare_results_to_graph(file, ty, \"ml\"))\n",
    "    rel = load_data(\"JM-{}-Relation.csv\".format(file[0]))\n",
    "    stats.append(classify_sents(rel, file))\n",
    "\n",
    "prec = []\n",
    "recall = []\n",
    "f1 = []\n",
    "for row in stats:\n",
    "    prec.append(row[4])\n",
    "    recall.append(row[5])\n",
    "    f1.append(row[6])\n",
    "print(\"Avg prec Score: \" + str(Average(prec)) + \"\\nAvg recall Score: \" + \n",
    "      str(Average(recall)) + \"\\nAvg F1 Score: \" + str(Average(f1)))\n",
    "\n",
    "stats_df = pd.DataFrame(stats, columns =['Filename', 'Predictions', 'Graph', 'True Positives', 'Prec', 'Recall', 'F1'])\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "nlp1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
